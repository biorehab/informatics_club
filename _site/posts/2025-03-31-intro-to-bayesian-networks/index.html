<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://fonts.googleapis.com/css2?family=Comfortaa:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/informatics_club/assets/css/main.css">
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        }
      };
    </script>
    <script async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <title>Introduction to Bayesian Networks</title>
</head>
<body>
    <main>
      <div class="main-content">
        <header>
          <a class="post-head-link" href="/informatics_club/">‚Üê Informatics Club</a>
        </header>
        <h2 class="post-title">Introduction to Bayesian Networks</h2>
        <p class="post-meta">
          <strong>Published on:</strong> March 31, 2025<br>
          <strong>Author:</strong> Sivakumar Balasubramanian
        </p>
        <hr>
        <div class="post-content">
          <link rel="stylesheet" href="/informatics_club/assets/css/2025-03-31-bn.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/11.11.1/math.min.js"></script>
<script>
  // Wait for the page to fully reload after live-reload
  document.addEventListener("DOMContentLoaded", function () {
    if (window.MathJax && window.MathJax.typeset) {
      MathJax.typeset();
    }
  });
</script>
<!-- Bayes Rule Discrete Interactive Demo -->
<!-- ==================================== -->
<script>
  // Event handlers
  // Function to update the corresponding span label
  function updateSliderLabel(slider) {
      const valueDisplay = document.getElementById(slider.id + "-value");
      if (valueDisplay) {
          valueDisplay.textContent = slider.value;
      }
  }

  // Function to handle checkbox state change
  function updateTestCheckboxLabel(checkBox) {
      document.getElementById("binary-test-result-value").textContent = checkBox.checked ? "Test is +ve" : "Test is -ve";
  }

  // Posterior probability calculation
  function updatePosteriorProbability() {
      // Get the slider values
      const pPrior = parseFloat(document.getElementById("prior-prob-slider").value);
      const pTruePos = parseFloat(document.getElementById("true-positive-slider").value);
      const pFalsePos = parseFloat(document.getElementById("false-positive-slider").value);
      const testResult = document.getElementById("binaryTestResult").checked ? 1 : 0;

      // Compute proability of the test result.
      pTest = calculateTestProbability(testResult);

      // Calculate the posterior probability
      var pPosterior = testResult == 1 ?  (pTruePos * pPrior) / pTest : ((1 - pTruePos) * pPrior) / pTest;
      document.getElementById("posterior-prob-value").textContent = `p(D=1 | T=${testResult}) = ${pPosterior.toFixed(5)}`;
  }

  // Compute test probability
  function calculateTestProbability(testResult) {
      // Get the slider values
      const pPrior = parseFloat(document.getElementById("prior-prob-slider").value);
      const pTruePos = parseFloat(document.getElementById("true-positive-slider").value);
      const pFalsePos = parseFloat(document.getElementById("false-positive-slider").value);

      // Calculate the probability of the test result
      const pTest = pTruePos * pPrior + pFalsePos * (1 - pPrior);
      return testResult == 1 ? pTest : 1 - pTest;
  }

  // Functions to read and update the Baye rule demo div segment's controls.
  document.addEventListener("DOMContentLoaded", function () {
      // Select all sliders
      const sliders = document.querySelectorAll(".slider");
      // Select the checkbox
      const testResultCheckbox = document.getElementById("binaryTestResult");

      // Attach event listeners to all sliders
      sliders.forEach(slider => {
          // Initialize labels with the current values
          updateSliderLabel(slider);

          // Update value when the slider is moved
          slider.addEventListener("input", function () {
              updateSliderLabel(slider);
              // Call the posterior probability function
              updatePosteriorProbability();
          });
      });

      // Initialize label for the checkbox. 
      updateTestCheckboxLabel(testResultCheckbox);

      // Attach an event listener
      testResultCheckbox.addEventListener("change", function() {
        updateTestCheckboxLabel(testResultCheckbox);
        // Call the posterior probability function
        updatePosteriorProbability();
      });

      // Call the posterior probability function
      updatePosteriorProbability();
  });
</script>
<p>Almost all real-world problems require us to deal with: (a) multiple variables with complex interactions, (b) each variable has some uncertainty associated with it, and (c) some decision making based on some partially observed or available information. Bayesian networks are a powerful tool that can help us represents such complex systems, and provide principled approaches for make informed decisions. In this post, we will provide a short introduction to Bayesian networks, and show how they can be used to model complex systems and make decisions under uncertainty. Bayesian networks are a type of probabilistic graphical model and are a stepping stone to causal inference --  an interesting, complex, and useful topic of interest to the author.</p>
<h2 class="post-subtitle">Some basic probability concepts</h2>
<p>Let's start by stating some basic probabilistic concepts. Let a random variable $X$ which takes on some values from its domain $\text{dom}(X)$. Let $0 \leq p(X = x) \leq 1$ is the probability that the random variable $X$ takes on the value $x \in \text{dom}(X)$.</p>
<p>$$
\sum_{x_i \in \text{dom}(X)} p(x_i) = 1
$$</p>
<p>This is the normalization condition. If $X$ is a continuous random variable, this condition is $$\int_{-\infty}^{\infty} p(x) dX = 1$$</p>
<p>When we have two random variables $X$ and $Y$, the <i>joint probability distribution</i> is denoted by $p(X=x, Y=y)$ or $p(x, y)$. The joint probability distribution is the probability that both $X$ and $Y$ take on the values $x$ and $y$, respectively.</p>
<p>The <i>marginal</i> of a joint distribution is given by,</p>
<p>$$
p(x) = \sum_{y} p(x, y) \quad \text{and} \quad p(y) = \sum_{x} p(x, y)
$$</p>
<p>Marginal probabilities tell us the probability of the individual random variables, irrespective of the other. <i>Conditional probability</i> tells us the probability of one random variable, when the value of another random variable is known. The conditional probability of $X$ given $Y$ is denoted by $p(X=x \mid Y=y)$ or $p(x \mid y)$ and is given by,</p>
<p>$$
p\left(x  \mid  y\right) = \frac{p\left(x, y\right)}{p\left( y \right)}, \quad p(y) \neq 0
$$</p>
<p>The reader must note the following points.</p>
<ol>
<li>$p( x \mid y)$ can be thought of a function of $x$ for a given vaues of $y$.</li>
<li>$\sum_{x} p( x  \mid  y) = 1$, which mean that $p(x  \mid  y)$ is a valid probability distribution. This property is due to the normalization of the joint probability distribution by $p( y )$.</li>
<li>$p(y) \neq 0$ is necessary because, if $p(y) = 0$ then then the random variable $Y$ cannot take on the value, so the conditional probability is not defined. Here is a simple example to understand conditional probability.</li>
</ol>
<div class = "example-box">
<p><strong>Example 1:</strong> Let's assume we have two identical bags - $B1$ and $B2$. $B1$ contains 10 red balls and $B2$ contains 10 blue balls. If we randomly choose between $B1$ and $B2$, with equal probability, and choose a ball from the chosen bag. What is the probability that the chosen ball is red? This would be $p( \text{ball} = red) = 0.5$. Why?</p>
<p>Now if you are told that the chosen bag is $B1$, then what is the probability that the ball is red with chosen bag being $B1$? $p( \text{ball} = red  \mid  \text{bag} = B1) = 1$! Why?</p>
<p>Note what happened. If we had not information about anything in this problem, the probability of choosing a red ball is 0.5. That is we are equally uncertain about which ball was chosen. However, if we find out which bag was chosen, then our uncertainty changes. In fact, in this case we are certain that the chosen ball is red, since the bag $B1$ was chosen.</p>
<p>What would happen if we learned instead that the chosen bag is $B2$?</p>
</div>
<p><strong> Baye's Rule </strong> is a very important concept in probability theory, and a crucial part of Bayesian networks. The names Bayesian networks comes from the fact that they are based on Baye's rule. Baye's rule is given by,</p>
<p>$$
p( x  \mid  y) = \frac{p( y  \mid  x) p(x)}{p(y)} \quad, p( y ) \neq 0
$$</p>
<p>This very simple rule has numerous applications, and in fact has an intuitive interpretation. The rule tells us how to update our belief or the uncertainity about an event $x$ given some evidence $y$. $p(x)$ is called the <i>prior</i> probability, which is a measure of belief about the event $x$ without any other information. $p(y  \mid  x)$ is the <i>likelihood</i> of the evidence $y$ given $x$, which is our belief about observing the data or information $y$ if event $x$ happens. $p(y)$ is the <i>marginal likelihood</i> of the evidence, and $p(x  \mid  y)$ is the <i>posterior</i> probability of $x$ given $y$. The posterior probability is our updated belief about the event $x$ given the evidence $y$.</p>
<h3 class="post-subsubtitle">Baye's Rule Interactive Demo</h3>
<p>Let's look at an interactive demonstration of Baye's rule. The following interactive demo of a commonly used &quot;medical&quot; example of the Baye's rule. We have a subject who take a test $T$ for a disease $D$. The test outcome and the disease state are binary random variables; the test outcome is positive (1) or negative (0), and the subject can either have (1) or not have (0) the disease. The disease has an incidence rate of $p(D = 1)$ in the population - the <i>prior probability</i>. The test for the disease is not perfect; it has a some known <i>true positive rate</i> $p(T = 1  \mid  D = 1)$ and a <i>false positive rate</i> $p(T = 1  \mid  D = 0)$. The following demo allows us to compute the <i>posterior probability</i> of the subject having the disease after we know the test result, i.e., $p( D = 1  \mid  T = 1)$ or $p( D = 1  \mid  T = 0)$. In the following interactive demo, you can change the prior probability of the disease, the true positive rate, the false positive rate, and the test result to see how the posterior probability changes.</p>
<div id="bayes-rule-discrete-demo">
  <!-- Prior Probability -->
  <div class="bndiscdemo-section">
    <div class="bndiscdemo-title">Prior Probability</div>
    <div class="bndiscdemo-controls">
      <label for="prior-prob-slider">$p(D=1)$</label>
      <input type="range" min="0" max="1" value="0.5" step="0.01" class="slider" id="prior-prob-slider">
      <span id="prior-prob-slider-value">0.5</span>
    </div>
  </div>
  <!-- True Positive Rate -->
  <div class="bndiscdemo-section">
    <div class="bndiscdemo-title">True Positive Rate</div>
    <div class="bndiscdemo-controls">
      <label for="true-positive-slider">$p(T = 1 \mid D=1)$</label>
      <input type="range" min="0" max="1" value="0.5" step="0.01" class="slider" id="true-positive-slider">
      <span id="true-positive-slider-value">0.5</span>
    </div>
  </div>
  <!-- False Positive Rate -->
  <div class="bndiscdemo-section">
    <div class="bndiscdemo-title">False Positive Rate</div>
    <div class="bndiscdemo-controls">
      <label for="false-positive-slider">$p(T = 1 \mid D=0)$</label>
      <input type="range" min="0" max="1" value="0.5" step="0.01" class="slider" id="false-positive-slider">
      <span id="false-positive-slider-value">0.5</span>
    </div>
  </div>
  <!-- Test Result (Checkbox) -->
  <div class="bndiscdemo-section">
    <div class="bndiscdemo-title">Test Result</div>
    <div class="bndiscdemo-controls" id="bndiscdemo-test-result">
      <label>
        <input type="checkbox" id="binaryTestResult"> $T$ Positive?
      </label>
      <span id="binary-test-result-value">-</span>
    </div>
  </div>
</div>
<!-- Posterior Probability Output -->
<div class="bndiscdemo-output">
  Posterior Probability: <span id="posterior-prob-value"> - </span>
</div>
<p>You can play around with the sliders above to compute the posterior probability of a person having the disease when the test comes out positive or negative. Answer the following questions using the interactive demo to get an intuitive understanding of Baye's rule.</p>
<div class="question-box">
<ol class="question">
  <li>When does the posterior probability equal the prior probability? Can you explain why it is so?</li>
  <li>When does the test result perfectly correlate with disease status? i.e., testing positive confirms the disease and vice versa. What about the other way around? Positive test implies no disease, and vice versa.</li>
</ol>
</div>
<p>Baye's rule can be used to update to obtain the full posterior probability distribution of a random variable we are interested in given some evidence. Suppose you find a coin on the street and we want to know if this is a fair coin. The coin looks like a regular coin so you believe that this coin is likely to be a fair coin, with the following prior probability distribution for the parameter $p$ - the probability of the coin landing heads up. Notice, here that $p$ is itself a random variable because of our uncertainty about its exact value. All we know is that its value is between 0 and 1. The Beta function is used to model the prior distribution of the parameter $p$. The Beta distribution is a continuous probability distribution defined on the interval $[0, 1]$. The Beta distribution is defined by two parameters $\alpha$ and $\beta$, which allow one to control the shape of the distribution. We toss the coin twenty times and depending on the number of head observed from this experiment, the posterior distribution changes. The following interactive demo shows how the prior (light red) and posterior (blue) distribution of the parameter $p$ changes as we observe more heads.</p>
<div class="container" id="bayesrule-coin-demo">
    <!-- Left: Controls -->
    <div class="controls">
        <h3>Bayesian Coin Flip</h3>
        <label>Prior Distribution Parameter Œ± (Heads): <span id="prior-alpha-value">2</span>
        <input type="range" id="prior-alpha" min="1" max="10" value="2"></label>
        <label>Prior Distribution Parameter Œ≤ (Tails): <span id="prior-beta-value">2</span>
        <input type="range" id="prior-beta" min="1" max="10" value="2"></label>
        <label>Observed Heads: <span id="obs-heads-value">0</span>
        <input type="range" id="obs-heads" min="0" max="20" value="10"></label>
    </div>
    <!-- Right: Chart -->
    <div class="chart-container">
        <svg width="500" height="250" viewBox="0 0 500 280" preserveAspectRatio="xMidYMid meet"></svg>
    </div>
</div>
<!-- Bayes Rule Coin Interactive Demo -->
<!-- ================================ -->
<script>
    // Function to compute the Beta distribution
    function betaPDF(x, alpha, beta) {
        function gamma(n) { return n === 1 ? 1 : (n - 1) * gamma(n - 1); }
        const B = (gamma(alpha) * gamma(beta)) / gamma(alpha + beta);
        return (Math.pow(x, alpha - 1) * Math.pow(1 - x, beta - 1)) / B;
    }

    // Set up SVG canvas
    const svg = d3.select(".chart-container svg"),
          width = +svg.attr("width"),
          height = +svg.attr("height"),
          margin = { top: 10, right: 10, bottom: 20, left: 30 };
    const plotWidth = width - margin.left - margin.right;
    const plotHeight = height - margin.top - margin.bottom;
    const g = svg.append("g").attr("transform", `translate(${margin.left},${margin.top})`);

    // Scales
    const xScale = d3.scaleLinear()
                     .domain([0, 1])
                     .range([0, plotWidth]);
    const yScale = d3.scaleLinear()
                     .domain([0, 5])
                     .range([plotHeight, 0]); // Adjusts dynamically

    // X Axis
    g.append("g")
     .attr("transform", `translate(0,${plotHeight})`)
     .attr("class", "x-axis")
     .call(d3.axisBottom(xScale))
     .selectAll("text") // Select all tick labels
     .style("font-size", "14px")  // Set font size
     .style("font-family", "Inter")  // Set font type
     .style("fill", "black");  // Set text color;
    // X-axis Label
    g.append("text")
     .attr("class", "x-axis-label")
     .attr("x", plotWidth / 2)
     .attr("y", plotHeight + margin.bottom + 20)
     .attr("text-anchor", "middle")
     .style("font-size", "16px")
     .style("font-family", "Inter")
     .style("fill", "black")
     .text("Probability of Heads (p)");
    // Y Axis
    g.append("g")
     .attr("class", "y-axis")
     .call(d3.axisLeft(yScale))
     .selectAll("text") // Select all tick labels
     .style("font-size", "14px")  // Set font size
     .style("font-family", "Inter")  // Set font type
     .style("fill", "black");  // Set text color;;

    // Line generators
    const line = d3.line()
                   .x(d => xScale(d.x))
                   .y(d => yScale(d.y));

    // Paths for prior and posterior
    const priorPath = g.append("path")
                       .attr("fill", "none")
                       .attr("stroke", "red")
                       .attr("stroke-width", 2)
                       .attr("stroke-opacity", 0.5);
    const posteriorPath = g.append("path")
                           .attr("fill", "none")
                           .attr("stroke", "steelblue")
                           .attr("stroke-width", 3);
    const legend = g.append("g")
                    .attr("transform", `translate(20, 10)`); // Position legend

    // Define legend items
    const legendItems = [
      { label: "Prior PDF", color: "red", strokeWidth: 2, opacity: 0.5 },
      { label: "Posterior PDF", color: "steelblue", strokeWidth: 3, opacity: 1 }
    ];

    // Create a group for each legend item
    const legendGroups = legend.selectAll(".legend-item")
                               .data(legendItems)
                               .enter()
                               .append("g")
                               .attr("class", "legend-item")
                               .attr("transform", (d, i) => `translate(${i * 100}, 0)`); // Space out items

    // Add colored legend lines
    legendGroups.append("line")
                .attr("x1", 0)
                .attr("y1", 5)
                .attr("x2", 20)
                .attr("y2", 5)
                .attr("stroke", d => d.color)
                .attr("stroke-width", d => d.strokeWidth)
                .attr("stroke-opacity", d => d.opacity);

    // Add text labels
    legendGroups.append("text")
                .attr("x", 30)
                .attr("y", 10)
                .style("font-size", "14px")
                .style("font-family", "Inter")  // Set font type
                .text(d => d.label);
    
    function updatePlot() {
        // Read slider values
        const priorAlpha = +document.getElementById("prior-alpha").value;
        const priorBeta = +document.getElementById("prior-beta").value;
        const obsHeads = +document.getElementById("obs-heads").value;
        const N = +document.getElementById("obs-heads").max;
        const obsTails = N - obsHeads;

        // Compute posterior parameters
        const postAlpha = priorAlpha + obsHeads;
        const postBeta = priorBeta + obsTails;

        // Update displayed values
        document.getElementById("prior-alpha-value").textContent = priorAlpha;
        document.getElementById("prior-beta-value").textContent = priorBeta;
        document.getElementById("obs-heads-value").textContent = `${obsHeads}/${N}`;

        // Generate data points for prior and posterior
        const data = d3.range(0.001, 1.001, 0.001).map(x => ({
            x,
            prior: betaPDF(x, priorAlpha, priorBeta),
            posterior: betaPDF(x, postAlpha, postBeta)
        }));

        // Update y-scale
        yScale.domain([0, d3.max(data, d => Math.max(d.prior, d.posterior)) * 1.2]);

        // Update axes
        svg.select(".y-axis")
           .transition()
           .duration(200)
           .call(d3.axisLeft(yScale))
           .selection()
           .selectAll("text")
           .style("font-size", "14px")
           .style("font-family", "Inter")
           .style("fill", "black");

        // Update prior plot
        priorPath.datum(data)
                 .transition()
                 .duration(200)
                 .attr("d", d3.line().x(d => xScale(d.x)).y(d => yScale(d.prior)));

        // Update posterior plot
        posteriorPath.datum(data)
                     .transition()
                     .duration(200)
                     .attr("d", d3.line().x(d => xScale(d.x)).y(d => yScale(d.posterior)));
    }

    // Event Listeners
    d3.selectAll("input").on("input", updatePlot);

    // Initial plot
    updatePlot();
</script>
<p>Play around with the sliders above and observe the changes in the prior and posterior distribution of the parameter $p$. Do the results make intuitive sense?</p>
<h2 class="post-subtitle">Mutlivariate problems with uncertainty</h2>
<p>With that brief background on Baye's theorem we now head towards the main topic of this post.</p>
<p>Let's assume that we are dealing with $n$ random variables $X_1, X_2, \ldots, X_n$ for the problem of interest. Given that the problem is stochastic in nature, the best piece of information we can have about the problem is the joint probability distribution of these random variables, i.e. $p\left( X_1, X_2, \cdots X_n\right)$. All other information that can be known about this problem can be derived from the joint probability distribution. If want to use a computer to compute the information of interest, we first need to be able to represent and store the joint probability distribution. That is the first issue we need to confront -  the representation of this joint probability distribution in a computer. Let's answers the following question to understand this issue:</p>
<div class="question-box">
<ul class="question">
  <li>How many parameters (or numbers) do you need to represent a discrete probability distribution of a random variable $X$ that takes on $k$ possible discrete values $x_1, x_2, x_3 \ldots x_k$?</li>
</ul>
</div>
<p>It's $k-1$ numbers, which can be $p(x_1), p(x_1), \ldots p(x_{k-1})$. The last number is determined by the normalization condition, i.e. $\sum_{i=1}^{k} p(x_i) = 1$. Now, let's consider multivariate.<br></p>
<div class="question-box">
<ul class="question">
  <li>How many parameters do you need to represent a discrete joint probability distribution of a two binary random variable $X_1$ and $X_2$ that take on values $0$ or $1$?</li>
</ul>
</div>
<p>Here, we will need 3 numbers. In fact, if we have a joint probability distribution of $n$ binary random variables, then, we need at most $2^n -1$ numbers to fully represent this joint probability distribution.</p>
<p>You see the problem here. The number of parameters required grows exponentially with the number of variables. If we were dealing with a moderately sized problem with 100 binary random variables, we will need $2^{100} - 1 = 1,267,650,600,228,229,401,496,703,205,375$ parameters! The is a ridiculously large number. Forget about using such a distribution for inference, we cannot even represent this fully on a computer!</p>
<div class="question-box">
<ul class="question">
  <li>What about a joint distribution of $n$ variables with each random variable taking on $k$ distinct values? I leave this for you to work out, and you will see the general issue here.</li>
</ul>
</div>
<p>For $n$ binary random variables, we said we will at most need $2^n - 1$ numbers to represent the probability distribution. This is when there is dependence between $n$ random variables, which requires all $2^n - 1$ numbers to capture all the possible dependencies or interactions. However, if there is any sort of &quot;independence&quot; between one or more of the random variables, then the number of parameters requires to represent the joint probability distribution can be reduced. To put this idea on firm footing, let's first clearly define the concept of independence in probability setting.</p>
<p><b>Independence</b></p>
<p>Two random variables $X$ and $Y$ are independent if the knowledge or information about one variable does not affect our knowledge about the other. This is most naturally expressed in terms of conditional probability. Two random variables are independent if the conditional probability distribution of one random variable given the other is equal to its marginal probability distribution.
$$
p(x \mid y) = p(x) \quad \forall x, y
$$</p>
<p>Our uncertainity about the value of $X$ without any other information is $p(X=x)$. Knowing the values of $Y$ does not change our uncertainty about the value of $X$, when $X$ and $Y$ are independent. Note that independence is a symmetric property.
$$
p(x \mid y) = p(x) \iff p(y \mid x) = p(y) \quad \forall x, y
$$
This implies that, $p(x, y) = p(x) p(y)$, $\forall x, y$. Its trivial to verify this. Independence between random variables $X$ is often represented as $X \perp\!\!\!\perp Y$. When two two random variables are not independent, we say that they are dependent, which is represented as $X \not\!\perp \!\!\!\perp Y$.</p>
<p>This independence is also known as <i>unconditional independence</i>. Another important and interesting type of independence is <i>conditional independence</i>.</p>
<p>Let's look at some examples to understand the concept of independence.</p>
<div class="example-box">
<p><strong>Example 2:</strong> The age of a first year MS Bioengineering student and his/her grade in Applied Linear Algebra course are two random variables. The age of the student does not affect his/her grade in the course. Thus, these two random variables are independent. Similarly, the gender of the student and his/her grade in the course are independent.</p>
<p><strong>Example 3:</strong> A person's age and his/her blood group can be safely assumed to be independent random variables, i.e.
$$p(\text{Age} \mid \text{Blood Type}) = p(\text{Age}), \,\, \forall \text{Age}, \,\text{Blood Type}$$
The following is the demonstration of this from a real dataset from <a href="https://www.kaggle.com/datasets/prasad22/healthcare-dataset/versions/1?resource=download#">Kaggle</a> with data from 10000 subjects. The following plot shows the conditional probability distribution of the age of the subjects given their blood type, along with the marginal probability distribution of the age. We see that these distributions look essentially the same; they will never be exactly equal because of sampling noise. A $\chi^2$ test of independence on the data shows that the two random variables are independent.</p>
<!-- Image from the analysis folder with center alignment -->
<img src="/informatics_club/assets/images/2025-03-31/age-bloodtype.svg" alt="Independence" class="example-image" width="400" style="display: block; margin: 0 auto;">
<p><strong>Example 4:</strong>Two Gaussian random variables $X$ and $Y$ are independent if and only if their joint distribution is given by the product of their marginals. If we randomly sample $X$ and $Y$ from a joint Gaussian distribution, we get a scatter plot as the following left plot, where there is appear to be no trend or correlation between the two random variables.</p>
<!-- Image from the analysis folder with center alignment -->
<img src="/informatics_club/assets/images/2025-03-31/gaussrandvar.svg" alt="Independence" class="example-image" width="500" style="display: block; margin: 0 auto;">
<p>On the other hand, the scatter plot on the right shows a case where the two random variables are dependent. Can you explain why these using the basic definition of independence?</p>
</div>
<p><b>Conditional Independence</b></p>
<p>Two variables $X$ and $Y$ may not be independent, but might become independent when we have knowledge of a third variable $Z$. The uncertainty about $X$ given $Y$ and $Z$ is the same as the uncertainty about $X$ given only $Z$.
$$
p(x \mid y, z) = p(x \mid z) \quad \forall x, y, z
$$
Notice, that conditional independence is also symmetric.
$$
p(x \mid y, z) = p(x \mid z) \iff p(y \mid x, z) = p(y \mid z) \quad \forall x, y, z
$$</p>
<p>This also implies that $p(x, y \mid z) = p(x \mid z) p(y \mid z)$, $\forall x, y, z$. Conditional independence is often represented as $X \perp\!\!\!\perp Y \mid Z$.</p>
<p>Let's look at some examples to understand the concept of independence.</p>
<div class="example-box">
<p><strong>Example 5:</strong> Two repeated administration of a diagnostic test $T$ on a subject are not independent. The outcomes in the two tests $T_1$ (test 1) and $T_2$ (test 2) are random variables and they will be correlated. Let's simulate this and understand this. Let this diagnostic test be a binary test with a true positive rate of $0.9$ and a false positive rate of $0.05$. Let the prevalence of the disease of interest be $0.1$. We administered this test twice on 1000 subjects who came to the hospital. Each of these patients either has or does not have the disease, which too is a random variable, which we call $D$. Comparing the outcomes of the two tests, we get the following joint probability distribution on the left. The right plot shows the conditional probability distribution of $T_2$, given the outcome of $T_1$.</p>
<!-- Image from the analysis folder with center alignment -->
<img src="/informatics_club/assets/images/2025-03-31/testretestcm1.svg" alt="Independence" class="example-image" width="450" style="display: block; margin: 0 auto;">
<p>We can clearly see that the $p(T_2 \mid T_1) \neq p(T_2)$, implying they are not unconditionally independent. If someone tested positive in the first test, they are more likely to test positive on the second test as well.</p>
<p>Ask yourself this question. If through some means, we got to know the true disease status of a subject, are the two tests $T_1$ and $T_2$ still dependent? Let's look at what the data says. The following plots show the conditional probability distribution $p(T_2 \mid T_1, D=1)$ and $p(T_2 \mid T_1, D=0)$, on the left and right, respectively. You can see that knowledge of $T_1$ once we know the disease does not change our uncertainty about $T_2$, which is essentially $p(T_2)$.</p>
<!-- Image from the analysis folder with center alignment -->
<img src="/informatics_club/assets/images/2025-03-31/testretestcm2.svg" alt="Independence" class="example-image" width="450" style="display: block; margin: 0 auto;">
<p>One we know the disease status, we cannot learn anything about the results of $T_2$ from $T_1$.</p>
</div>
<h2 class="post-subtitle">Representation of Multivariate Distributions</h2>
<p>We earlier saw that a joint distribution with $N$ binary random variables requires at most $2^{N}-1$ variables for its complete specification. This is the most general case, where each of the $N$ variables is dependent on other random variables. If these names these random variables $X_1, X_2, \ldots, X_N$, then the joint distribution can be written as the following,
$$
p(x_1, x_2, \ldots, x_N) = p(x_N \mid x_{N-1}, x_{N-2}, \ldots, x_1) p(x_{N-1} \mid x_{N-2}, \ldots, x_1) \cdots p(x_{2} \mid x_1) p(x_1)
$$
This is the <i>chain rule</i> of probability distributions, where the joint distribution is expressed as the product of a set of conditional distributions. The chain rule written above is the most general form of the rule, which allows for all possible dependencies between the random variables. If we can specify each of these conditional distributions, then we can compute the joint distribution. There are $N$ product terms on the right hand side (RHS), and its easy to verify that the sum of the parameters required for each of these conditional distribution is $2^N - 1$. The following table shows the number of parameters required to represent the joint distribution of $N$ binary random variables, and the number of product terms in the chain rule.</p>
<div class="centered-table">
<table>
  <thead>
    <tr>
      <th>Term in the RHS</th>
      <th>Number of Parameters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$p(x_1)$</td>
      <td>$1$</td>
    </tr>
    <tr>
      <td>$p(x_2 \mid x_1)$</td>
      <td>$2$</td>
    </tr>
    <tr>
      <td>$p(x_3 \mid x_2, x_1)$</td>
      <td>$4$</td>
    </tr>
    <tr>
      <td>$\vdots$</td>
      <td>$\vdots$</td>
    </tr>
    <tr>
      <td>$p(x_N \mid x_{N-1}, x_{N-2}, \ldots, x_1)$</td>
      <td>$2^{N-1}$</td>
    </tr>
  </tbody>
</table>
</div>
<p>You can verify that $\sum_{i=0}^{N-1} 2^i = 2^N - 1$.</p>
<p>What would happen if the $N$ random variables are independent? If each one is independent, then the joint distribution can be written as the product of the marginals.
$$
p(x_1, x_2, \ldots, x_N) = p(x_1) p(x_2) \cdots p(x_N)
$$</p>
<p>This is the simplest possible case. We only need $N$ parameters to represent the joint distribution.</p>
<p>Often in practice, we neither have full independence or full dependence between the random variables. Conditional independence between variables betows structure to a joint distribution, allowing us to have a compact representation for joint probability distributions.</p>
<h2 class="post-subtitle">Bayesian Networks</h2>
<p><b>Bayesian Networks</b> are a nice, graphical way to represent conditional independence between random variables, that allows us to compactly represent joint distributions, and provides algorithms for making probabilistic inferences from data. Bayesian networks are also often called <i>belief networks</i> or <i>Bayesian belief networks</i>.</p>
<p>A Bayesian network is a directed acyclic graph (DAG) where the nodes are random variables and the edges represent the conditional dependencies between the random variables. The direction of the edge indicates the direction of the dependency. The choice of the edges in a Bayesian network are based on the conditional independence relationships between the random variables of interest.</p>
<p>The following figures shown some examples of Bayesian networks (BNs) with three random variables.</p>
<div class="centered-table-noborder">
<table>
  <thead>
    <tr>
      <th>BN 1</th>
      <th>BN 2</th>
      <th>BN 3</th>
      <th>BN 4</th>
      <th>BN 5</th>
    </tr>
  </thead>
  <tbody>
    </tr>
    <tr>
      <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph1.svg" alt="Graph1" class="graph1" width="150" style="display: block; margin: 0 auto;">
      </td>
      <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph2.svg" alt="Graph1" class="graph1" width="150" style="display: block; margin: 0 auto;">
      </td>
      <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph3.svg" alt="Graph1" class="graph1" width="150" style="display: block; margin: 0 auto;">
      </td>
      <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph4.svg" alt="Graph1" class="graph1" width="150" style="display: block; margin: 0 auto;">
      </td>
      <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph5.svg" alt="Graph1" class="graph1" width="150" style="display: block; margin: 0 auto;">
      </td>
    </tr>
  </tbody>
</table>
</div>
<p>The BNs shown above are five possible examples of BNs with three random variables. The following some graph therapy definitions that will be useful when talking about BNs:</p>
<ol style="color: rgb(100, 100, 100)">
  <li><b>Node:</b> The circles (with text) shown in the above graphs.</li>
  <li><b>Edge:</b> The arrows between two nodes of the graph. The arrows represent the conditional dependencies between the random variables; the direction of the arrow is often based on causal dependency, but this is not necessary in BN.
  <li><b>Directed path:</b> A <i>directed path</i> from node $A$ to node $B$ is a sequence of nodes $A = v_0 \rightarrow v_1 \rightarrow v_2 \rightarrow \cdots \rightarrow v_k = B$ such that each consecutive pair $(v_i, v_{i+1})$ has a directed edge in the graph (i.e., the arrow points from $v_i$ to $v_{i+1}$).
  <li><b>Child node:</b> A node $B$ is a <i>child</i> of $A$, if there is an arrow (directed edge) from $A$ to $B$, i.e. $A \longrightarrow B$. For example, in BN2, $X_2$ and $X_3$ are children of $X_3$.
  <li><b>Parent node:</b> A node $A$ is a <i>parent</i> of $B$, if there is an arrow (directed edge) from $A$ to $B$, i.e. $A \longrightarrow B$. For example, in BN3, $X_1$ is the parent of $X_2$ and $X_2$ is parent of $X_3$. The set of all nodes that are parents of a node $X$ is represented by $\text{pa}(X)$. In BN4, $\text{pa}(X_2) = \{ X_1 \}$, and in BN5, $\text{pa}(X_1) = \{ X_2, X_3\}$.
  <li><b>Ancestor node:</b> A node $A$ is an <i>ancestor</i> of a node $B$, if there is a directed path from $A$ to $B$.</li> In BN3, the nodes $X_1$ and $X_2$ are ancestors of $X_3$.
  <li><b>Markov blanket:</b> A <i>Markov blanket</i> of a node is the set of all nodes that that include its parents, its children, and the other parents of its children.</li>
</ol>
<p>The joint probability distribution of a BN can be written as the product of the conditional probability distributions of each node given its parents.
$$
p(x_1, x_2, \ldots, x_N) = \prod_{i=1}^{N} p(x_i \mid \text{pa}(x_i))
$$
$\text{pa}(x_i)$ is the set of all parent nodes of node $x_i$. The number of parameters required to represent the joint distribution of a BN is equal to the sum of the number of parameters required to represent each conditional distribution. Assuming all random variables are binary, the number of parameters required to represent a conditional distribution $p(x_i \mid \text{pa}(x_i))$ is equal to $2^{\#\text{pa}(x_i)} - 1$, where $\#\text{pa}(x_i)$ is the number of parent nodes of node $x_i$.</p>
<p>We are now ready analyze the BNs shown above.</p>
<table class="bn-table">
  <tr class="bn-row">
    <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph1.svg" alt="BN1" class="bn-image">
    </td>
    <td class="bn-text">
      All nodes are independent, unconditionally.
      $$
      p(x_1, x_2, x_3) = p(x_1)p(x_2)p(x_3), \quad \text{No. of params: } 3
      $$
<p><p style="color: black"><b>Conditional Independence:</b></p> $$
X_1 \perp\!\!\!\perp X_2, \,\, X_1 \perp\!\!\!\perp X_3, \,\, X_2 \perp\!\!\!\perp X_3
$$
</td></p>
  </tr>
  <tr class="bn-row">
    <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph2.svg" alt="BN2" class="bn-image">
    </td>
    <td class="bn-text">
      There is dependency between all nodes in this BN.
      $$
      p(x_1, x_2, x_3) = p(x_3 \mid x_2, x_1)p(x_2 \mid x_1)p(x_1), \quad \text{No. of params: } 7
      $$
<p><p style="color: black"><b>Conditional Independence:</b> None</p>
</td></p>
  </tr>
  <tr class="bn-row">
    <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph3.svg" alt="BN3" class="bn-image">
    </td>
    <td class="bn-text">
      <!-- <div class="bn-title">BN 3:</div> -->
      There is only partial dependency between the nodes in this BN.
      $$
      p(x_1, x_2, x_3) = p(x_3 \mid x_2)p(x_2 \mid x_1)p(x_1), \quad \text{No. of params: } 5
      $$
<p><p style="color: black"><b>Conditional Independence:</b></p> $$
X_1 \perp\!\!\!\perp X_3  \mid  X_2
$$
</td></p>
  </tr>
  <tr class="bn-row">
    <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph4.svg" alt="BN4" class="bn-image">
    </td>
    <td class="bn-text">
      <!-- <div class="bn-title">BN 4:</div> -->
      There is only partial dependency between the nodes in this BN.
      $$
      p(x_1, x_2, x_3) = p(x_3 \mid x_1)p(x_2 \mid x_1)p(x_1), \quad \text{No. of params: } 5
      $$
<p><p style="color: black"><b>Conditional Independence:</b></p> $$
X_2 \perp\!\!\!\perp X_3  \mid  X_1
$$
</td></p>
  </tr>
  <tr class="bn-row">
    <td>
      <img src="/informatics_club/assets/images/2025-03-31/graph5.svg" alt="BN5" class="bn-image">
    </td>
    <td class="bn-text">
      <!-- <div class="bn-title">BN 5:</div> -->
      There is only partial dependency between the nodes in this BN.
      $$
      p(x_1, x_2, x_3) = p(x_3 \mid x_2, x_1)p(x_2)p(x_1), \quad \text{No. of params: } 5
      $$
<p><p style="color: black"><b>Conditional Independence:</b></p> $$
X_1 \perp\!\!\!\perp X_2
$$
</td></p>
  </tr>
</table>
<p>Comparing the probability distribution of BN1, BN3-5 with BN2, we find that missing edges between nodes corresponds to missing variables in the local conditional probability distribution.</p>
<p>The last three BNs shown above are three important canonical graphs involving three random variables: BN3, BN4, and BN5. We will take a closer look at these, since a good understanding of these three simple BNs will be useful in understanding more complex BNs.</p>
<div class="bn-container">
  <div class="bn-description">
<p><b>BN3</b> $(X_1 \longrightarrow X_2 \longrightarrow X_3)$: Here, $X_1$ is conditionally independent of $X_3$ for a given value of $X_2$. Thus, we can think of this as if $X_2$ is blocking the flow of information between $X_1$ and $X_3$. A simple example (from <a href="https://people.eecs.berkeley.edu/~jordan/prelims/chapter2.pdf" target="_blank">Chapter 02</a> of <i>Introduction to Probabilistic Graphical Models</i> by <i>Michael I. Jordan</i>) is where $X_1$, $X_2$, and $X_3$ represent the past, present, and the future. The past is independent of the future given the present.</p>
<p>The adjacent plots demonstrates this through an interactive demo. A simple linear model is assumed between the three random variables:
$$
X1 = \epsilon_1, \quad X2 = 2 X1 + \epsilon_2, \quad X3 = -X2 + \epsilon_3
$$</p>
<p>where, $\epsilon_1, \epsilon_2 \sim \mathcal{N}(0, 2)$ and $\epsilon_3 \sim \mathcal{N}(0, 3)$. When we don't condition, $X_1$ and $X_2$ are correlated (blue dots); but when we condition on $X_2$, $X_1$ and $X_3$ are independent.</p>
  </div>
  <div class="bn-plot-area">
    <div class="bn-controls">
      <label>
        <input type="checkbox" id="conditionCheckboxBN3">
        Condition on $X_2$
      </label>
      <label>
        <input type="range" id="bSliderBN3" min="-10" max="10" step="0.1" value="0">
        <span id="bValueBN3">0.0</span>
      </label>
    </div>
    <svg class="BNDemo" id="BN3Demo" width="300" height="250"></svg>
  </div>
</div>
<script src="/informatics_club/assets/js/2025-03-31-bn3demo.js" defer></script>
<div class="bn-container">
  <div class="bn-description">
<p><b>BN4</b> $(X_2 \longleftarrow X_1 \longrightarrow X_3)$: Here, $X_1$ is conditionally independent of $X_3$ for a given value of $X_2$. Thus, we can think of this as if $X_2$ is blocking the flow of information between $X_1$ and $X_3$. Example 5 discussed above represents this, where $X_1 = D$, $X_2 = T_1$, and $X_3 = T_2$. $T_1$ and $T_2$ are independent given the disease status $D$. The node $X_1$ blocks the flow of information between $X_2$ and $X_3$. $X_1$ is a like a hidden or latent variable that &quot;causes&quot; both $X_2$ and $X_3$.</p>
<p>The adjacent plots demonstrates this through an interactive demo. A simple linear model is assumed between the three random variables:
$$
X1 = \epsilon_1, \quad X2 = 2 X1 + \epsilon_2, \quad X3 = 3X2 + \epsilon_3
$$</p>
<p>where, $\epsilon_1 \sim \mathcal{N}(0, 3)$, $\epsilon_2 \sim \mathcal{N}(0, 2)$ and $\epsilon_3 \sim \mathcal{N}(0, 4)$. When we don't condition, $X_2$ and $X_3$ are correlated (blue dots); but when we condition on $X_1$, $X_2$ and $X_3$ are independent.</p>
  </div>
  <div class="bn-plot-area">
    <div class="bn-controls">
      <label>
        <input type="checkbox" id="conditionCheckboxBN4">
        Condition on $X_1$
      </label>
      <label>
        <input type="range" id="bSliderBN4" min="-10" max="10" step="0.1" value="0">
        <span id="bValueBN4">0.0</span>
      </label>
    </div>
    <svg class="BNDemo" id="BN4Demo" width="300" height="250"></svg>
  </div>
</div>
<!-- X1 -> X2 -> X3 -->
<script src="/informatics_club/assets/js/2025-03-31-bn4demo.js" defer></script>
<div class="bn-container">
  <div class="bn-description">
<p><b>BN5</b> $(X_1 \longrightarrow X_3 \longleftarrow X_2)$: This is an interesting scenario. $X_1$ and $X_2$ are unconditionally independent. However, if we know the value of $X_3$, then $X_1$ and $X_2$ are become dependent. This is called <i>explaining away</i>. A simple example of this is the following. Let $X_3 = X_1 + X_2$, where $X_1$ and $X_2$ can take on any value real value between $0$ and $1$. When we don't know the value of $X_3$, $X_1$ and $X_2$ can take on any value between $0$ and $1$, and are independent of each other. But once we know the value of $X_3$, knowledge of $X_1$ tells us the value of $X_2$, and vice versa. Normally, the flow of information is blocked by the node $X_3$, which is also called a <i>collider</i>, because two or more arrows point to the node. A collider will usually block the flow of information between its parents, unless we conditions on the collider or its descendants. When we condition, the collider opens the flow of information between its parents.</p>
<p>The adjacent plots demonstrates this through an interactive demo. A simple linear model is assumed between the three random variables:
$$
X1 = \epsilon_1, \quad X2 = \epsilon_2, \quad X3 = 1.5X_1 + X2 + \epsilon_3
$$</p>
<p>where, $\epsilon_1 \sim \mathcal{N}(0, 3)$, $\epsilon_2 \sim \mathcal{N}(0, 4)$ and $\epsilon_3 \sim \mathcal{N}(0, 2)$. When we don't condition, $X_1$ and $X_2$ are independent (blue dots); but when we condition on $X_3$, $X_1$ and $X_2$ are dependent.</p>
  </div>
  <div class="bn-plot-area">
    <div class="bn-controls">
      <label>
        <input type="checkbox" id="conditionCheckboxBN5">
        Condition on $X_1$
      </label>
      <label>
        <input type="range" id="bSliderBN5" min="-20" max="20" step="0.1" value="0">
        <span id="bValueBN5">0.0</span>
      </label>
    </div>
    <svg class="BNDemo" id="BN5Demo" width="300" height="250"></svg>
  </div>
</div>
<!-- X1 -> X3 <- X2 -->
<script src="/informatics_club/assets/js/2025-03-31-bn5demo.js" defer></script>
<p>Equipped with this basic understanding, we will now move onto a slightly more complex BN shown below.</p>
<center><img src="/informatics_club/assets/images/2025-03-31/graph6.svg" alt="BN5" width=300></center>
<p>This BN has five random variables: $X_1$, $X_2$, $X_3$, $X_4$, and $X_5$. Can you write down the joint probability distribution for this BN?</p>
<details>
  <summary>Click to see the answer</summary>
<p>$$
p(x_1, x_2, x_3, x_4, x_5) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1) p(x_4 \mid x_3, x_2) p(x_5 \mid x_4)
$$</p>
<ul>
<li>
<p>$X_1$ has no parents</p>
</li>
<li>
<p>$X_2$ and $X_3$ have $X_1$ as a parent</p>
</li>
<li>
<p>$X_4$ has two parents $X_2$ and $X_3$</p>
</li>
<li>
<p>$X_5$ has a single parent $X_4$</p>
</li>
</ul>
</details>
<p>What can you say about the conditional independence of the random variables in this BN? Can you list all of them?</p>
<details>
  <summary>Click to see the answer</summary>
<ul>
<li>
<p>$X_2 \perp\!\!\!\perp X_3 \mid X_1$ : $X_2$ and $X_3$ are conditionally independent given $X_1$.</p>
</li>
<li>
<p>$X_4 \perp\!\!\!\perp X_1 \mid \{ X_2, X_3 \}$: $X_4$ is conditionally independent of $X_1$ given $X_2$ <strong>AND</strong> $X_3$.</p>
</li>
<li>
<p>$X_5 \perp\!\!\!\perp \{X_1, X_2, X_3 \} \mid X_4$: $X_5$ is conditionally independent of $X_1$, $X_2$, <strong>AND</strong> $X_3$ given $X_4$.</p>
</li>
</ul>
</details>
<p>Let's now simulate this network and see if the conditional independence we've lsited above holds. Note that this is not the proof for the correctness of the conditional independence, but it is a reasonable sanity check. You should be able to verify a conditional independence statement by simply using the joint distribution and computing the conditional probabilities.</p>
<div class="bn-plot-area">
    <div class="bn-controls">
      <!-- X1 controls -->
      <label>
        <input type="checkbox" id="conditionCheckboxBN6X1">
        $X_1$
      </label>
      <label>
        <input type="range" id="bSliderBN6X1" min="-6" max="6" step="0.1" value="0">
        <span id="bValueBN6X1">0.0</span>
      </label>
      <!-- X2 controls -->
      <label>
        <input type="checkbox" id="conditionCheckboxBN6X2">
        $X_2$
      </label>
      <label>
        <input type="range" id="bSliderBN6X2" min="-10" max="10" step="0.1" value="0">
        <span id="bValueBN6X2">0.0</span>
      </label>
      <!-- X3 controls -->
      <label>
        <input type="checkbox" id="conditionCheckboxBN6X3">
        $X_3$
      </label>
      <label>
        <input type="range" id="bSliderBN6X3" min="-10" max="10" step="0.1" value="0">
        <span id="bValueBN6X3">0.0</span>
      </label>
      <!-- X4 controls -->
      <label>
        <input type="checkbox" id="conditionCheckboxBN6X4">
        $X_4$
      </label>
      <label>
        <input type="range" id="bSliderBN6X4" min="-10" max="10" step="0.1" value="0">
        <span id="bValueBN6X4">0.0</span>
      </label>
      <!-- X5 controls -->
      <label>
        <input type="checkbox" id="conditionCheckboxBN6X5">
        $X_5$
      </label>
      <label>
        <input type="range" id="bSliderBN6X5" min="-10" max="10" step="0.1" value="0">
        <span id="bValueBN6X5">0.0</span>
      </label>
    </div>
    <svg class="BNDemo" id="BN6Demo" width="865 " height="865"></svg>
</div>
<script src="/informatics_club/assets/js/2025-03-31-bn6demo.js" defer></script>
<p>You can now try the following by yourself:</p>
<center><img src="/informatics_club/assets/images/2025-03-31/graph7.svg" alt="BN5" width=400></center>
<p>In progress ...</p>

        </div>
      </div>
    </main>
</body>
</html>